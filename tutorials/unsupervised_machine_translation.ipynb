{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised vs. Supervised Machine Translation\n",
    "\n",
    "In this tutorial, we'll use Adaptor to look at the **difference in accuracy** when training domain-specific translator using standard **Supervised** vs. **Unsupervised objectives**. We'll also use one extra domain to estimate the distributional **robustness** of our translator throughout the adaptation.\n",
    "\n",
    "* For the supervised adaptation and evaluations, we'll use standard *Sequence2Sequence* (i.e. MLE) objective.\n",
    "* For the unsupervised adaptation, we'll use Adaptor's unsupervised *BackTranslation* objectice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SB2KDo6-XP7l",
    "outputId": "c4b6be6a-af86-42f9-d433-d13e2c94d7aa"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "\n",
    "!git clone https://github.com/gaussalgo/adaptor.git\n",
    "!pip install -e adaptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the utils directory to the working directory, so we can easily import it\n",
    "!mv adaptor/examples ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset resolution\n",
    "\n",
    "We will use Adaptor's `OPUSDataset` wrapper available in [/examples](https://github.com/gaussalgo/adaptor/blob/master/examples/data_utils_opus.py) of github repo. For a supported set of domains, this wrapper will download or reload the cached dataset and parse it into `source` and `target` list of strings. New domains can be added by [adding their urls](https://github.com/gaussalgo/adaptor/blob/db33e6e439babc68fe801a8946d87116ff44f170/examples/data_utils_opus.py#L10) from [https://opus.nlpl.eu](https://opus.nlpl.eu/).\n",
    "\n",
    "Note that `data_utils_opus.py` also takes care of deduplicating the datasets - if the sample of given source text was already loaded, it will be skipped in the next-loaded `OPUSDataset`s. This is to make sure that no data leakage between train and validation splits exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping 82264 duplicates from wikimedia train split.\n",
      "Dropping 10283 duplicates from wikimedia val split.\n",
      "Dropping 10284 duplicates from wikimedia test split.\n",
      "Dropping 33877148 duplicates from OpenSubtitles train split.\n",
      "Dropping 4234644 duplicates from OpenSubtitles val split.\n",
      "Dropping 4234644 duplicates from OpenSubtitles test split.\n"
     ]
    }
   ],
   "source": [
    "from examples.data_utils_opus import OPUSDataset\n",
    "\n",
    "src_lang = \"cs\"\n",
    "tgt_lang = \"en\"\n",
    "\n",
    "data_dir = \"examples\"\n",
    "\n",
    "val_size = 100\n",
    "test_size = 1000\n",
    "\n",
    "wiki_pairs = OPUSDataset(\"wikimedia\", \"train\", src_lang, tgt_lang, data_dir=data_dir)\n",
    "wiki_val_pairs = OPUSDataset(\"wikimedia\", \"val\", src_lang, tgt_lang, data_dir=data_dir, firstn=val_size)\n",
    "wiki_test_pairs = OPUSDataset(\"wikimedia\", \"test\", src_lang, tgt_lang, data_dir=data_dir, firstn=test_size)\n",
    "\n",
    "opensub_pairs = OPUSDataset(\"OpenSubtitles\", \"train\", src_lang, tgt_lang, data_dir=data_dir, firstn=val_size)\n",
    "opensub_val_pairs = OPUSDataset(\"OpenSubtitles\", \"val\", src_lang, tgt_lang, data_dir=data_dir, firstn=val_size)\n",
    "opensub_test_pairs = OPUSDataset(\"OpenSubtitles\", \"test\", src_lang, tgt_lang, data_dir=data_dir, firstn=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ukrajinský lékař ortoped-traumatologist nejvyšší kategorie, MUDr, vedoucí výzkumník z kloubních onemocnění u dospělých státní instituce \"Instituce traumatologie a ortopedie NAMS Ukrajiny\"',\n",
       " 'Martha Elefteriadu (* 12. září 1946 Bulkes, Jugoslávie) je česká zpěvačka řeckého původu, polovina pěveckého dua Martha a Tena, které již od mládí tvoří se svojí sestrou Tenou Elefteriadu.',\n",
       " 'Životopis',\n",
       " 'Pochází z rodiny řeckých emigrantů, jež uprchla z Řecka kvůli občanské válce a usadila se v roce 1950 v někdejším Československu.',\n",
       " 'Jejich maminka zemřela brzy v dětství a obě dívky vyrůstaly v dětském domově pro řecké děti v Ivančicích.',\n",
       " 'Martha po maturitě na gymnáziu studovala nejprve medicinu, později přestoupila na studium psychologie, které dokončila na Karlově univerzitě v Praze.',\n",
       " 'Koncem šedesátých let se obě sestry seznamují s kytaristou Alešem Sigmundem ze skupiny Vulkán, který oběma zpěvačkám pomáhá vytvořit pevné autorské a muzikantské zázemí.',\n",
       " 'Jejich první nahrávky pocházejí z roku 1968, v roce 1970 vydávají svoji první LP dlouhohrající desku u vydavatelství Panton Dál než slunce vstává.',\n",
       " 'Od té doby jsou obě sestry stálicemi české populární hudby.',\n",
       " 'Za dobu svého působení zde spolupracovaly s celou řadou renomovaných umělců.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_pairs.source[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^ this is a default format of the **samples** of Sequence2Sequence and inherited objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ukrainian doctor orthopedic traumatologist highest category, Doctor of Medical Science, senior researcher of joint disease of adults of government institution \"Institute of Traumatology and Orthopedics NAMS Ukraine\"',\n",
       " 'Martha a Tena is a Czech music duo whose members are sisters Martha Elefteriadu and Tena Elefteriadu. Martha Elefteriadu (* 12. září 1946 Bulkes, Jugoslávie) is a Czech singer of řeckého původu, half of the duo Martha a Tena, together with her sister Tena Elefteriadu. Tena Elefteriadu, born as Partena Elefteriadu (*16.',\n",
       " 'Personal Life',\n",
       " 'Their family emigrated from Řecka because of the občanské válce and settled in 1950 in former Československu.',\n",
       " 'Their mother died while they were children, so they grew up in a dětském domově for Greek immigrants in Ivančicích.',\n",
       " 'Martha, after maturitě at a gymnáziu studied first general medicine, then changed major to psychologie, and graduated from Karlově univerzitě in Prague.',\n",
       " \"Tena has a son Marko Elefteriadis, a rapper who performs under stage name Ektor. Career At the end of the 1960's the sisters met a guitarist Aleš Sigmund from band Vulkán, who helped them create strong autorské and muzikantské foundations.\",\n",
       " 'Their first records are from 1968, in 1970 they released their first LP dlouhohrající desku with Panton Dál než slunce vstává.',\n",
       " 'They quickly established themselves in Czech populární hudby.',\n",
       " 'They collaborated with a series of famous artists.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_pairs.target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^ this is a default format of the **labels** of Sequence2Sequence and inherited objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our base translator for adaptation, we pick a general Helsinki-NLP model. This model has an architecture of the Transformer-base and has been pre-trained on a bulk dump of a subset of OPUS domains. Likely, it has already been exposed to our domains of adaptation and evaluation (Wiki & OpenSubtitles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptor.lang_module import LangModule\n",
    "\n",
    "lang_module = LangModule(\"Helsinki-NLP/opus-mt-%s-%s\" % (src_lang, tgt_lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the training, we evaluate model's BLEU. \n",
    "\n",
    "Using the identical interface, Adaptor allows you to evaluate any of other generative measures, that can better fit your task. Alternatively, you can relatively easily also implement your own generative evaluator. \n",
    "Take a look [here](https://github.com/gaussalgo/adaptor/blob/db33e6e439babc68fe801a8946d87116ff44f170/adaptor/evaluators/generative.py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptor.evaluators.generative import BLEU\n",
    "\n",
    "evaluators = [BLEU(additional_sep_char=\"▁\", decides_convergence=True)]  # \"▁\" is a specific separation token that sometimes relains left after output decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised adaptation\n",
    "\n",
    "In the first experiment, we use standard Sequence2Sequence (also used under a name MLE -Maximum Likelihood Estimation- objective). This objective maximises probability of every subsequent token under the assuptions of given input and the coreectly-generated preceding output.\n",
    "\n",
    "Adaptor Objectives provide high-level interface, expecting both the input texts and output texts (=labels) in a form of:\n",
    "* either a `List[str]`, with the texts and labels of the matching length\n",
    "* or a paths to a `.txt` files with one sample / label per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptor.objectives.seq2seq import Sequence2Sequence\n",
    "\n",
    "seq_wiki = Sequence2Sequence(lang_module,\n",
    "                             texts_or_path=wiki_pairs.source,\n",
    "                             labels_or_path=wiki_pairs.target,\n",
    "                             val_texts_or_path=wiki_val_pairs.source,\n",
    "                             val_labels_or_path=wiki_val_pairs.target,\n",
    "                             source_lang_id=src_lang,\n",
    "                             target_lang_id=tgt_lang,\n",
    "                             batch_size=8,\n",
    "                             val_evaluators=evaluators,\n",
    "                             objective_id=\"Wiki\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same interface, we also instantiate objectives used only for evaluation. \n",
    "\n",
    "Note that in order to avoid initialising a separate head of the shared model, you need to pass `share_other_objective_head` argument with a reference to other objective that will fully share its model with the new objective. The head of the evaluation objective would otherwise never be tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 15:31:49 | WARNING | root | Objective Opensub-Sequence2Sequence will use SEQ2SEQ head of Wiki-Sequence2Sequence objective\n",
      "2022-05-18 15:31:50 | WARNING | root | These layers of the loaded SEQ2SEQ were not merged: []\n"
     ]
    }
   ],
   "source": [
    "eval_opensub = Sequence2Sequence(lang_module,\n",
    "                                 texts_or_path=opensub_pairs.source,\n",
    "                                 labels_or_path=opensub_pairs.target,\n",
    "                                 val_texts_or_path=opensub_val_pairs.source,\n",
    "                                 val_labels_or_path=opensub_val_pairs.target,\n",
    "                                 source_lang_id=src_lang,\n",
    "                                 target_lang_id=tgt_lang,\n",
    "                                 batch_size=8,\n",
    "                                 val_evaluators=evaluators,\n",
    "                                 share_other_objective_head=seq_wiki,\n",
    "                                 objective_id=\"Opensub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are done with the datasets, objectives and evaluators, we set up the `AdaptationArguments`. These are a small extension of 🤗 's [TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer?highlight=launch#transformers.TrainingArguments), the extra parameters are documented with [AdaptationArguments definition](https://github.com/gaussalgo/adaptor/blob/db33e6e439babc68fe801a8946d87116ff44f170/adaptor/utils.py#L77)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "from adaptor.utils import AdaptationArguments, StoppingStrategy\n",
    "\n",
    "training_arguments = AdaptationArguments(output_dir=\"experiments\",\n",
    "                                         learning_rate=2e-5,\n",
    "                                         stopping_strategy=StoppingStrategy.ALL_OBJECTIVES_CONVERGED,\n",
    "                                         stopping_patience=5,\n",
    "                                         do_train=True,\n",
    "                                         do_eval=True,\n",
    "                                         warmup_steps=5000,\n",
    "                                         gradient_accumulation_steps=4,\n",
    "                                         logging_steps=100,\n",
    "                                         eval_steps=100,\n",
    "                                         save_steps=1000,\n",
    "                                         num_train_epochs=10,\n",
    "                                         evaluation_strategy=\"steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a `Schedule`, defining an order of application of selected `Objective`s. If our training is a single-objective, we can pick any Schedule available - it makes no difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a main object called `Adapter`: this is again merely a small adjustment of 🤗  [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer?highlight=launch#transformers.Trainer), that takes care of data iteration according to selected `Schedule`, collection of `Objective`s' logs or applying selected multi-objective early-stopping strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 15:31:50 | WARNING | root | Total number of train samples: 73835\n",
      "2022-05-18 15:31:50 | WARNING | root | Total number of eval samples: 200\n"
     ]
    }
   ],
   "source": [
    "from adaptor.schedules import SequentialSchedule\n",
    "from adaptor.adapter import Adapter\n",
    "\n",
    "schedule = SequentialSchedule(objectives=[seq_wiki],\n",
    "                              extra_eval_objectives=[eval_opensub],\n",
    "                              args=training_arguments)\n",
    "adapter = Adapter(lang_module, schedule, args=training_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 92290\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 230720\n",
      "2022-05-18 15:31:54 | WARNING | root | Converged objectives: []\n",
      "Wiki-Sequence2Sequence:   4%|▍         | 400/9229 [00:51<17:09,  8.57batches/s, epoch=1, loss=4.65, split=train] 2022-05-18 15:32:46 | WARNING | root | Converged objectives: []\n",
      "2022-05-18 15:32:46 | WARNING | root | Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4617, 'learning_rate': 4.0000000000000003e-07, 'train_Wiki-Sequence2Sequence_loss': 3.1017318576574326, 'train_Wiki-Sequence2Sequence_num_batches': 20, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence: 100%|██████████| 12/12 [00:00<00:00,  9.92batches/s, epoch=1, loss=1.46, split=eval]\n",
      "Opensub-Sequence2Sequence:   0%|          | 0/12 [00:00<?, ?batches/s]\u001B[A\n",
      "Opensub-Sequence2Sequence:  50%|█████     | 6/12 [00:00<00:00, 58.02batches/s, epoch=0, loss=0.966, split=eval]\u001B[A\n",
      "Wiki-Sequence2Sequence: 13batches [00:01, 10.74batches/s, epoch=1, loss=4.32, split=eval]                   al]\u001B[A\n",
      "Opensub-Sequence2Sequence: 13batches [00:15,  1.19s/batches, epoch=0, loss=0.737, split=eval]                  \n",
      "2022-05-18 15:33:06 | WARNING | root | Converged objectives: []\n",
      "Wiki-Sequence2Sequence:   4%|▍         | 402/9229 [01:12<9:37:00,  3.92s/batches, epoch=1, loss=3.73, split=train] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6360028982162476, 'eval_runtime': 1.2126, 'eval_samples_per_second': 164.935, 'eval_steps_per_second': 164.935, 'eval_Wiki-Sequence2Sequence_loss': 2.132140645614037, 'eval_Wiki-Sequence2Sequence_num_batches': 13, 'eval_Wiki-Sequence2Sequence_BLEU-gen': 40.082749105757074, 'eval_Opensub-Sequence2Sequence_loss': 1.1398652287629933, 'eval_Opensub-Sequence2Sequence_num_batches': 13, 'eval_Opensub-Sequence2Sequence_BLEU-gen': 33.1747919975654, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence:   9%|▊         | 799/9229 [02:07<19:51,  7.08batches/s, epoch=1, loss=1.52, split=train]  2022-05-18 15:34:02 | WARNING | root | Converged objectives: []\n",
      "2022-05-18 15:34:02 | WARNING | root | Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5779, 'learning_rate': 8.000000000000001e-07, 'train_Wiki-Sequence2Sequence_loss': 2.8302815437316893, 'train_Wiki-Sequence2Sequence_num_batches': 20, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence:   0%|          | 0/12 [01:14<?, ?batches/s, epoch=1, loss=-1, split=eval]\n",
      "Wiki-Sequence2Sequence: 100%|██████████| 12/12 [00:00<00:00,  9.66batches/s, epoch=1, loss=1.44, split=eval]\n",
      "Opensub-Sequence2Sequence:   0%|          | 0/12 [01:00<?, ?batches/s, epoch=0, loss=-1, split=eval]\n",
      "\n",
      "Opensub-Sequence2Sequence:  50%|█████     | 6/12 [00:00<00:00, 57.51batches/s, epoch=0, loss=0.958, split=eval]\u001B[A\n",
      "Wiki-Sequence2Sequence: 13batches [00:01, 10.41batches/s, epoch=1, loss=4.29, split=eval]                   al]\u001B[A\n",
      "\n",
      "Opensub-Sequence2Sequence: 13batches [00:14,  1.15s/batches, epoch=0, loss=0.727, split=eval]                  \u001B[A\n",
      "2022-05-18 15:34:22 | WARNING | root | Converged objectives: []\n",
      "Wiki-Sequence2Sequence:   9%|▊         | 803/9229 [02:27<6:38:52,  2.84s/batches, epoch=1, loss=2.35, split=train] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6195905208587646, 'eval_runtime': 1.2517, 'eval_samples_per_second': 159.785, 'eval_steps_per_second': 159.785, 'eval_Wiki-Sequence2Sequence_loss': 2.120600416110112, 'eval_Wiki-Sequence2Sequence_num_batches': 26, 'eval_Wiki-Sequence2Sequence_BLEU-gen': 40.468985817613984, 'eval_Opensub-Sequence2Sequence_loss': 1.1349929800400367, 'eval_Opensub-Sequence2Sequence_num_batches': 26, 'eval_Opensub-Sequence2Sequence_BLEU-gen': 33.83879196529923, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence:  13%|█▎        | 1200/9229 [03:20<15:16,  8.76batches/s, epoch=1, loss=3.56, split=train] 2022-05-18 15:35:15 | WARNING | root | Converged objectives: []\n",
      "2022-05-18 15:35:15 | WARNING | root | Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0675, 'learning_rate': 1.2000000000000002e-06, 'train_Wiki-Sequence2Sequence_loss': 2.020648795366287, 'train_Wiki-Sequence2Sequence_num_batches': 20, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence:   0%|          | 0/12 [01:11<?, ?batches/s, epoch=1, loss=-1, split=eval]\n",
      "Wiki-Sequence2Sequence: 100%|██████████| 12/12 [00:00<00:00,  9.52batches/s, epoch=1, loss=1.42, split=eval]\n",
      "Opensub-Sequence2Sequence:   0%|          | 0/12 [00:57<?, ?batches/s, epoch=0, loss=-1, split=eval]\n",
      "\n",
      "Opensub-Sequence2Sequence:  50%|█████     | 6/12 [00:00<00:00, 55.40batches/s, epoch=0, loss=0.948, split=eval]\u001B[A\n",
      "Wiki-Sequence2Sequence: 13batches [00:01, 10.31batches/s, epoch=1, loss=4.26, split=eval]                   al]\u001B[A\n",
      "Opensub-Sequence2Sequence: 13batches [00:17,  1.35s/batches, epoch=0, loss=0.707, split=eval]                  \n",
      "2022-05-18 15:35:38 | WARNING | root | Converged objectives: []\n",
      "Wiki-Sequence2Sequence:  13%|█▎        | 1201/9229 [03:43<12:56:49,  5.81s/batches, epoch=1, loss=4.17, split=train]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6039254665374756, 'eval_runtime': 1.2638, 'eval_samples_per_second': 158.254, 'eval_steps_per_second': 158.254, 'eval_Wiki-Sequence2Sequence_loss': 2.1103171752049374, 'eval_Wiki-Sequence2Sequence_num_batches': 39, 'eval_Wiki-Sequence2Sequence_BLEU-gen': 40.70180013944074, 'eval_Opensub-Sequence2Sequence_loss': 1.1293619382075775, 'eval_Opensub-Sequence2Sequence_num_batches': 39, 'eval_Opensub-Sequence2Sequence_BLEU-gen': 33.59675622896459, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence:  17%|█▋        | 1600/9229 [04:34<19:54,  6.39batches/s, epoch=1, loss=2.71, split=train]   2022-05-18 15:36:28 | WARNING | root | Converged objectives: []\n",
      "2022-05-18 15:36:28 | WARNING | root | Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0801, 'learning_rate': 1.6000000000000001e-06, 'train_Wiki-Sequence2Sequence_loss': 2.0120967388153077, 'train_Wiki-Sequence2Sequence_num_batches': 20, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence:   0%|          | 0/12 [01:12<?, ?batches/s, epoch=1, loss=-1, split=eval]\n",
      "Wiki-Sequence2Sequence: 100%|██████████| 12/12 [00:00<00:00,  9.64batches/s, epoch=1, loss=1.4, split=eval] \n",
      "Opensub-Sequence2Sequence:   0%|          | 0/12 [00:56<?, ?batches/s, epoch=0, loss=-1, split=eval]\n",
      "\n",
      "Opensub-Sequence2Sequence:  50%|█████     | 6/12 [00:00<00:00, 52.20batches/s, epoch=0, loss=0.939, split=eval]\u001B[A\n",
      "Wiki-Sequence2Sequence: 13batches [00:01, 10.20batches/s, epoch=1, loss=4.25, split=eval]                  val]\u001B[A\n",
      "\n",
      "Opensub-Sequence2Sequence: 13batches [00:22,  1.72s/batches, epoch=0, loss=0.69, split=eval]                   \u001B[A\n",
      "2022-05-18 15:36:56 | WARNING | root | Converged objectives: []\n",
      "Wiki-Sequence2Sequence:  17%|█▋        | 1602/9229 [05:02<12:03:57,  5.70s/batches, epoch=1, loss=6.33, split=train]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5907896757125854, 'eval_runtime': 1.2792, 'eval_samples_per_second': 156.343, 'eval_steps_per_second': 156.343, 'eval_Wiki-Sequence2Sequence_loss': 2.100764954319367, 'eval_Wiki-Sequence2Sequence_num_batches': 52, 'eval_Wiki-Sequence2Sequence_BLEU-gen': 40.686281717602974, 'eval_Opensub-Sequence2Sequence_loss': 1.1243891773315577, 'eval_Opensub-Sequence2Sequence_num_batches': 52, 'eval_Opensub-Sequence2Sequence_BLEU-gen': 33.59989581829224, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence:  22%|██▏       | 2000/9229 [05:55<28:28,  4.23batches/s, epoch=1, loss=3.29, split=train]   2022-05-18 15:37:49 | WARNING | root | Converged objectives: []\n",
      "2022-05-18 15:37:49 | WARNING | root | Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3024, 'learning_rate': 2.0000000000000003e-06, 'train_Wiki-Sequence2Sequence_loss': 3.0922449231147766, 'train_Wiki-Sequence2Sequence_num_batches': 20, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence:   0%|          | 0/12 [01:19<?, ?batches/s, epoch=1, loss=-1, split=eval]\n",
      "Wiki-Sequence2Sequence: 100%|██████████| 12/12 [00:00<00:00,  9.63batches/s, epoch=1, loss=1.38, split=eval]\n",
      "Opensub-Sequence2Sequence:   0%|          | 0/12 [00:58<?, ?batches/s, epoch=0, loss=-1, split=eval]\n",
      "\n",
      "Opensub-Sequence2Sequence:  50%|█████     | 6/12 [00:00<00:00, 51.90batches/s, epoch=0, loss=0.932, split=eval]\u001B[A\n",
      "Wiki-Sequence2Sequence: 13batches [00:01, 10.18batches/s, epoch=1, loss=4.23, split=eval]                   al]\u001B[A\n",
      "\n",
      "Opensub-Sequence2Sequence: 13batches [00:18,  1.45s/batches, epoch=0, loss=0.676, split=eval]                  \u001B[A\n",
      "2022-05-18 15:38:13 | WARNING | root | Converged objectives: []\n",
      "Wiki-Sequence2Sequence:  22%|██▏       | 2002/9229 [06:19<10:29:03,  5.22s/batches, epoch=1, loss=2.19, split=train]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5825088024139404, 'eval_runtime': 1.2816, 'eval_samples_per_second': 156.059, 'eval_steps_per_second': 156.059, 'eval_Wiki-Sequence2Sequence_loss': 2.0929031051122227, 'eval_Wiki-Sequence2Sequence_num_batches': 65, 'eval_Wiki-Sequence2Sequence_BLEU-gen': 40.74483165473145, 'eval_Opensub-Sequence2Sequence_loss': 1.120223727593055, 'eval_Opensub-Sequence2Sequence_num_batches': 65, 'eval_Opensub-Sequence2Sequence_BLEU-gen': 34.08044916118936, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence:  26%|██▌       | 2400/9229 [07:16<28:38,  3.97batches/s, epoch=1, loss=2.96, split=train]   2022-05-18 15:39:11 | WARNING | root | Converged objectives: []\n",
      "2022-05-18 15:39:11 | WARNING | root | Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7079, 'learning_rate': 2.4000000000000003e-06, 'train_Wiki-Sequence2Sequence_loss': 3.840408110618591, 'train_Wiki-Sequence2Sequence_num_batches': 20, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence:   0%|          | 0/12 [01:20<?, ?batches/s, epoch=1, loss=-1, split=eval]\n",
      "Wiki-Sequence2Sequence: 100%|██████████| 12/12 [00:00<00:00,  9.74batches/s, epoch=1, loss=1.37, split=eval]\n",
      "Opensub-Sequence2Sequence:   0%|          | 0/12 [01:02<?, ?batches/s, epoch=0, loss=-1, split=eval]\n",
      "\n",
      "Opensub-Sequence2Sequence:  50%|█████     | 6/12 [00:00<00:00, 57.67batches/s, epoch=0, loss=0.935, split=eval]\u001B[A\n",
      "Wiki-Sequence2Sequence: 13batches [00:01, 10.45batches/s, epoch=1, loss=4.21, split=eval]                   al]\u001B[A\n",
      "\n",
      "Opensub-Sequence2Sequence: 13batches [00:15,  1.21s/batches, epoch=0, loss=0.671, split=eval]                  \u001B[A\n",
      "2022-05-18 15:39:31 | WARNING | root | Converged objectives: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5757427215576172, 'eval_runtime': 1.2486, 'eval_samples_per_second': 160.176, 'eval_steps_per_second': 160.176, 'eval_Wiki-Sequence2Sequence_loss': 2.085502403668868, 'eval_Wiki-Sequence2Sequence_num_batches': 78, 'eval_Wiki-Sequence2Sequence_BLEU-gen': 40.566049366399504, 'eval_Opensub-Sequence2Sequence_loss': 1.1173508686897082, 'eval_Opensub-Sequence2Sequence_num_batches': 78, 'eval_Opensub-Sequence2Sequence_BLEU-gen': 34.30814750204521, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence:  30%|███       | 2799/9229 [08:29<11:44,  9.13batches/s, epoch=1, loss=1.96, split=train]   2022-05-18 15:40:24 | WARNING | root | Converged objectives: []\n",
      "2022-05-18 15:40:24 | WARNING | root | Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3469, 'learning_rate': 2.8000000000000003e-06, 'train_Wiki-Sequence2Sequence_loss': 2.4922434031963348, 'train_Wiki-Sequence2Sequence_num_batches': 20, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence:   0%|          | 0/12 [01:11<?, ?batches/s, epoch=1, loss=-1, split=eval]\n",
      "Wiki-Sequence2Sequence: 100%|██████████| 12/12 [00:00<00:00,  9.71batches/s, epoch=1, loss=1.37, split=eval]\n",
      "Opensub-Sequence2Sequence:   0%|          | 0/12 [00:57<?, ?batches/s, epoch=0, loss=-1, split=eval]\n",
      "\n",
      "Opensub-Sequence2Sequence:  50%|█████     | 6/12 [00:00<00:00, 57.50batches/s, epoch=0, loss=0.93, split=eval]\u001B[A\n",
      "Wiki-Sequence2Sequence: 13batches [00:01, 10.48batches/s, epoch=1, loss=4.2, split=eval]                    al]\u001B[A\n",
      "Opensub-Sequence2Sequence: 13batches [00:18,  1.44s/batches, epoch=0, loss=0.66, split=eval]                   \n",
      "2022-05-18 15:40:48 | WARNING | root | Converged objectives: []\n",
      "Wiki-Sequence2Sequence:  30%|███       | 2802/9229 [08:54<6:25:04,  3.59s/batches, epoch=1, loss=2.04, split=train]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5681531429290771, 'eval_runtime': 1.2444, 'eval_samples_per_second': 160.724, 'eval_steps_per_second': 160.724, 'eval_Wiki-Sequence2Sequence_loss': 2.078510567382142, 'eval_Wiki-Sequence2Sequence_num_batches': 91, 'eval_Wiki-Sequence2Sequence_BLEU-gen': 40.604685723278365, 'eval_Opensub-Sequence2Sequence_loss': 1.1148359710043603, 'eval_Opensub-Sequence2Sequence_num_batches': 91, 'eval_Opensub-Sequence2Sequence_BLEU-gen': 33.81431821416742, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence:  35%|███▍      | 3199/9229 [09:52<13:58,  7.19batches/s, epoch=1, loss=1.21, split=train]  2022-05-18 15:41:46 | WARNING | root | Converged objectives: []\n",
      "2022-05-18 15:41:46 | WARNING | root | Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.399, 'learning_rate': 3.2000000000000003e-06, 'train_Wiki-Sequence2Sequence_loss': 2.7137348473072054, 'train_Wiki-Sequence2Sequence_num_batches': 20, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence:   0%|          | 0/12 [01:21<?, ?batches/s, epoch=1, loss=-1, split=eval]\n",
      "Wiki-Sequence2Sequence: 100%|██████████| 12/12 [00:00<00:00,  9.76batches/s, epoch=1, loss=1.37, split=eval]\n",
      "Opensub-Sequence2Sequence:   0%|          | 0/12 [01:03<?, ?batches/s, epoch=0, loss=-1, split=eval]\n",
      "\n",
      "Opensub-Sequence2Sequence:  50%|█████     | 6/12 [00:00<00:00, 56.64batches/s, epoch=0, loss=0.933, split=eval]\u001B[A\n",
      "Wiki-Sequence2Sequence: 13batches [00:01, 10.44batches/s, epoch=1, loss=4.19, split=eval]                   al]\u001B[A\n",
      "Opensub-Sequence2Sequence: 13batches [00:15,  1.20s/batches, epoch=0, loss=0.66, split=eval]                   \n",
      "2022-05-18 15:42:07 | WARNING | root | Converged objectives: []\n",
      "Wiki-Sequence2Sequence:  35%|███▍      | 3202/9229 [10:12<5:03:39,  3.02s/batches, epoch=1, loss=1.35, split=train]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5618665218353271, 'eval_runtime': 1.2468, 'eval_samples_per_second': 160.413, 'eval_steps_per_second': 160.413, 'eval_Wiki-Sequence2Sequence_loss': 2.1069789761304856, 'eval_Wiki-Sequence2Sequence_num_batches': 100, 'eval_Wiki-Sequence2Sequence_BLEU-gen': 40.45699478308604, 'eval_Opensub-Sequence2Sequence_loss': 1.107856280207634, 'eval_Opensub-Sequence2Sequence_num_batches': 100, 'eval_Opensub-Sequence2Sequence_BLEU-gen': 34.267562743943515, 'epoch': 0.03}\n",
      "2022-05-18 21:05:15 | WARNING | root | Scheduler reached the given maximum number of epochs for all objectives. Triggering termination.\n",
      "2022-05-18 21:05:15 | WARNING | root | Scheduler reached a termination condition: ALL_OBJECTIVES_NUM_EPOCHS\n",
      "2022-05-18 21:05:15 | WARNING | root | Evaluating...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 1\n",
      "{'loss': 1.1921, 'learning_rate': 1.839624313308524e-05, 'train_Wiki-Sequence2Sequence_loss': 1.2053922072052956, 'train_Wiki-Sequence2Sequence_num_batches': 20, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wiki-Sequence2Sequence:   0%|          | 0/12 [01:49<?, ?batches/s, epoch=10, loss=-1, split=eval]\n",
      "Wiki-Sequence2Sequence:   8%|▊         | 1/12 [00:00<00:00, 35.24batches/s, epoch=11, loss=0.764, split=eval]\n",
      "Opensub-Sequence2Sequence:   0%|          | 0/12 [01:47<?, ?batches/s, epoch=0, loss=-1, split=eval]\n",
      "Wiki-Sequence2Sequence:   1%|          | 100/9229 [00:28<13:43, 11.08batches/s, epoch=11, loss=0.469, split=train]2022-05-18 21:05:37 | WARNING | root | Scheduler reached the given maximum number of epochs for all objectives. Triggering termination.\n",
      "2022-05-18 21:05:37 | WARNING | root | Scheduler reached a termination condition: ALL_OBJECTIVES_NUM_EPOCHS\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "2022-05-18 21:05:37 | WARNING | root | Scheduler reached the given maximum number of epochs for all objectives. Triggering termination.\n",
      "2022-05-18 21:05:37 | WARNING | root | Scheduler reached a termination condition: ALL_OBJECTIVES_NUM_EPOCHS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7644965052604675, 'eval_runtime': 0.0305, 'eval_samples_per_second': 261.927, 'eval_steps_per_second': 261.927, 'eval_Wiki-Sequence2Sequence_loss': 1.5943953630328178, 'eval_Wiki-Sequence2Sequence_num_batches': 100, 'eval_Wiki-Sequence2Sequence_BLEU-gen': 39.88782615413746, 'eval_Opensub-Sequence2Sequence_loss': 1.5059486478567123, 'eval_Opensub-Sequence2Sequence_num_batches': 100, 'eval_Opensub-Sequence2Sequence_BLEU-gen': 24.481207223719124, 'epoch': 1.0}\n",
      "{'train_runtime': 20023.1852, 'train_samples_per_second': 46.092, 'train_steps_per_second': 11.523, 'train_loss': 1.647409159342448, 'train_Wiki-Sequence2Sequence_loss': 1.2053922072052956, 'train_Wiki-Sequence2Sequence_num_batches': 20, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=23100, training_loss=1.647409159342448, metrics={'train_runtime': 20023.1852, 'train_samples_per_second': 46.092, 'train_steps_per_second': 11.523, 'total_flos': 0.0, 'train_loss': 1.647409159342448})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training terminates when the selected `StoppingStrategy` is satisfied. There is a slightly larger list of options to pick from, to cover the wider variety of multi-objective situations. See the [StoppingStrategy options](https://github.com/gaussalgo/adaptor/blob/db33e6e439babc68fe801a8946d87116ff44f170/adaptor/utils.py#L19).\n",
    "\n",
    "Let's quickly check if the training was terminated by a number of epochs (as the log says -- note the `Scheduler reached a termination condition` message in the log), or our `stopping_strategy=StoppingStrategy.ALL_OBJECTIVES_CONVERGED` (according to BLEU, since we've initialised it with `decides_convergence=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval BLEU\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.Series(seq_wiki.evaluations_history[\"eval\"][evaluators[0]]).plot(figsize=(15, 7), grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eval loss\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.Series(seq_wiki.evaluations_history[\"eval\"][\"loss\"]).plot(figsize=(15, 7), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised adaptation\n",
    "\n",
    "In this experiment we'll see hiw far can we get without a large set of aligned supervised texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-cs-en/resolve/main/config.json from cache at /home/xstefan3/.cache/huggingface/transformers/8574c5a6a6364b41827117f086812030143d2b25b92206d8b3f6d684745e8066.7bb9e3260d1abcad4452c2a159fd1668543f0b66b704f6ef4197dc58ccd92176\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      62508\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 62508,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 62508,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 62509\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-cs-en/resolve/main/source.spm from cache at /home/xstefan3/.cache/huggingface/transformers/de8aa4e2d80168b5e21926507904d37e0029e2cd9109288dc8bca1a53bf16fc2.f7b8f33623917afa192c98e8cdb7110a96e4ef9f1f49ad2548a62236c818d814\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-cs-en/resolve/main/target.spm from cache at /home/xstefan3/.cache/huggingface/transformers/a622ae1fe7c735110523a7462cefcb232a7c57d64bf782bb5e5b18d88199cd57.802f2650721ec7e21ef8b2ec0a6adbdfa24815d20e3bc4b2994a4e7323207ae2\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-cs-en/resolve/main/vocab.json from cache at /home/xstefan3/.cache/huggingface/transformers/cb57aa91d734e65f18e4a475380972973fc9e427cd781bb8017e5a65e00fbb40.12e5a9c0c42c3467972154314fa3a3c04b52a6735d1cd7216a6facbb0de44ce3\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-cs-en/resolve/main/tokenizer_config.json from cache at /home/xstefan3/.cache/huggingface/transformers/05a1c2f3d66206e92f99c7a3a7498ceafb8081ce786d4cc6a277da08e09599a0.c5753583caba2512f03c1dd8a50811762daadc90baedc6c606c7b137759e05e4\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-cs-en/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-cs-en/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-cs-en/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-cs-en/resolve/main/config.json from cache at /home/xstefan3/.cache/huggingface/transformers/8574c5a6a6364b41827117f086812030143d2b25b92206d8b3f6d684745e8066.7bb9e3260d1abcad4452c2a159fd1668543f0b66b704f6ef4197dc58ccd92176\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      62508\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 62508,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 62508,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 62509\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from adaptor.lang_module import LangModule\n",
    "\n",
    "lang_module = LangModule(\"Helsinki-NLP/opus-mt-%s-%s\" % (src_lang, tgt_lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `BackTranslation` objective, that first translates the target texts into the source language using given `BackTranslator` instance. This way, we do not need to provide the objective an aligned set of samples, but we still need unsupervised data of the target language.\n",
    "\n",
    "Of course, the eventual quality of such-trained model also heavily depends on a quaility of the BackTranslator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-en-cs/resolve/main/config.json from cache at /home/xstefan3/.cache/huggingface/transformers/827ff170f0b812bc7aeb5b56274c2626143bc2254efa5d4df89cec71c2fec9ac.7bb9e3260d1abcad4452c2a159fd1668543f0b66b704f6ef4197dc58ccd92176\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      62508\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 62508,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 62508,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 62509\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-cs/resolve/main/source.spm from cache at /home/xstefan3/.cache/huggingface/transformers/36561bb31c2702debc859e3fc9c11ac45adbb3a4744a0e2b52584026792a0441.802f2650721ec7e21ef8b2ec0a6adbdfa24815d20e3bc4b2994a4e7323207ae2\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-cs/resolve/main/target.spm from cache at /home/xstefan3/.cache/huggingface/transformers/dd1424e7a7dd9e5fe463a64e61e76c5ba9bfd10fd37050320ddc510cff8f57ba.f7b8f33623917afa192c98e8cdb7110a96e4ef9f1f49ad2548a62236c818d814\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-cs/resolve/main/vocab.json from cache at /home/xstefan3/.cache/huggingface/transformers/9865631fc3589f67952dcb08d792b7153a2017f43719cb789e2e982709c2177d.12e5a9c0c42c3467972154314fa3a3c04b52a6735d1cd7216a6facbb0de44ce3\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-cs/resolve/main/tokenizer_config.json from cache at /home/xstefan3/.cache/huggingface/transformers/394726bea97903848d60aae65c9f2689bd34c0b22a19f5c5b845f0e0187081d8.4aad148346bf95da5e433798f4585a3f0286cac6642930115fe59af0be15b5f9\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-cs/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-cs/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/Helsinki-NLP/opus-mt-en-cs/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-en-cs/resolve/main/config.json from cache at /home/xstefan3/.cache/huggingface/transformers/827ff170f0b812bc7aeb5b56274c2626143bc2254efa5d4df89cec71c2fec9ac.7bb9e3260d1abcad4452c2a159fd1668543f0b66b704f6ef4197dc58ccd92176\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      62508\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 62508,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 62508,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 62509\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-en-cs/resolve/main/config.json from cache at /home/xstefan3/.cache/huggingface/transformers/827ff170f0b812bc7aeb5b56274c2626143bc2254efa5d4df89cec71c2fec9ac.7bb9e3260d1abcad4452c2a159fd1668543f0b66b704f6ef4197dc58ccd92176\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      62508\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 62508,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 62508,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.10.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 62509\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Helsinki-NLP/opus-mt-en-cs/resolve/main/pytorch_model.bin from cache at /home/xstefan3/.cache/huggingface/transformers/8492de3383cb030e21a06a70f9c03669fdbce63c2ff0ece9a6175c8110720d9d.8771d1d8b1e0bfe440e0606c5f42988d8a31d2fbb37a1fa6b4a131d247363d7d\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-cs.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "2022-05-18 21:05:50 | WARNING | root | Objective Wiki-Back-BackTranslation will use SEQ2SEQ head of Wiki-Sequence2Sequence objective\n",
      "2022-05-18 21:05:50 | WARNING | root | Wiki-Back-BackTranslation expects target-language texts as input_texts_or_path. This is not further checked. \n"
     ]
    }
   ],
   "source": [
    "from adaptor.objectives.backtranslation import BackTranslation, BackTranslator\n",
    "\n",
    "backtrans_wiki = BackTranslation(lang_module,\n",
    "                                 back_translator=BackTranslator(\"Helsinki-NLP/opus-mt-%s-%s\" % (tgt_lang, src_lang)),\n",
    "                                 texts_or_path=wiki_pairs.target,\n",
    "                                 val_texts_or_path=wiki_val_pairs.target,\n",
    "                                 batch_size=8,\n",
    "                                 share_other_objective_head=seq_wiki,\n",
    "                                 objective_id=\"Wiki-Back\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the other pieces of puzzle remain the same. We'll initialise extra evaluation objective though, to be able to compare the logs of the two evaluation objectives based on their history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 21:05:50 | WARNING | root | Objective Opensub-Sequence2Sequence will use SEQ2SEQ head of Wiki-Sequence2Sequence objective\n",
      "2022-05-18 21:05:50 | WARNING | root | These layers of the loaded SEQ2SEQ were not merged: []\n"
     ]
    }
   ],
   "source": [
    "eval_opensub_unsup = Sequence2Sequence(lang_module,\n",
    "                                 texts_or_path=opensub_pairs.source,\n",
    "                                 labels_or_path=opensub_pairs.target,\n",
    "                                 val_texts_or_path=opensub_val_pairs.source,\n",
    "                                 val_labels_or_path=opensub_val_pairs.target,\n",
    "                                 source_lang_id=src_lang,\n",
    "                                 target_lang_id=tgt_lang,\n",
    "                                 batch_size=8,\n",
    "                                 val_evaluators=evaluators,\n",
    "                                 share_other_objective_head=seq_wiki,\n",
    "                                 objective_id=\"Opensub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-18 21:05:50 | WARNING | root | Total number of train samples: 73835\n",
      "2022-05-18 21:05:50 | WARNING | root | Total number of eval samples: 200\n"
     ]
    }
   ],
   "source": [
    "from adaptor.schedules import SequentialSchedule\n",
    "from adaptor.adapter import Adapter\n",
    "\n",
    "schedule = SequentialSchedule(objectives=[backtrans_wiki],\n",
    "                              extra_eval_objectives=[eval_opensub_unsup],\n",
    "                              args=training_arguments)\n",
    "adapter = Adapter(lang_module, schedule, args=training_arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though this training will take somewhat longer, since the back-translation of the samples in the first epoch is performed on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "adapter.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Let's see how the supervised adaptation is doing in comparison with the unsupervised BackTranslation objective.\n",
    "\n",
    "Thanks to Adaptor's separation to objectives, we can conveniently take a look at the validation accuracies of the two experiments, separately on in-distribution and out-of-distribution BLEU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wiki_bleus_sup = pd.Series(seq_wiki.evaluations_history['eval'][evaluators[0]])\n",
    "wiki_bleus_unsup = pd.Series(backtrans_wiki.evaluations_history['eval'][evaluators[0]])\n",
    "\n",
    "index = range(0, len(wiki_bleus_sup)*training_arguments.eval_steps, training_arguments.eval_steps)\n",
    "\n",
    "wiki_bleus_sup.index = index\n",
    "wiki_bleus_unsup.index = index\n",
    "\n",
    "wiki_bleus_sup.plot(figsize=(14, 5), grid=True, ylim=(0.75, 0.9), color=\"blue\", label=\"In-domain validation BLEU of supervised adaptation\")\n",
    "wiki_bleus_unsup.plot(figsize=(14, 5), grid=True, ylim=(0.75, 0.9), color=\"blue\", alpha=0.7, label=\"in-domain validation BLEU of unsupervised adaptation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensub_bleus_sup = pd.Series(eval_opensub.evaluations_history['eval'][evaluators[0]])\n",
    "opensub_bleus_unsup = pd.Series(eval_opensub_unsup.evaluations_history['eval'][evaluators[0]])\n",
    "\n",
    "index = range(0, len(opensub_bleus_sup)*training_arguments.eval_steps, training_arguments.eval_steps)\n",
    "\n",
    "opensub_bleus_sup.index = index\n",
    "opensub_bleus_unsup.index = index\n",
    "\n",
    "opensub_bleus_sup.plot(figsize=(14, 5), grid=True, ylim=(0.75, 0.9), color=\"blue\", label=\"Out-of-domain validation BLEU of supervised adaptation\")\n",
    "opensub_bleus_unsup.plot(figsize=(14, 5), grid=True, ylim=(0.75, 0.9), color=\"blue\", alpha=0.7, label=\"Out-of-domain validation BLEU of unsupervised adaptation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Finally, we evaluate the out models of both trainings, to see which performs better on a held-out set.\n",
    "\n",
    "Note, that in both cases, we've used `stopping_strategy=StoppingStrategy.ALL_OBJECTIVES_CONVERGED`, so the evaluated model is the model after passing this stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_model = seq_wiki.compatible_head_model\n",
    "unsupervised_model = backtrans_wiki.compatible_head_model\n",
    "\n",
    "evaluator = BLEU(additional_sep_char=\"▁\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bleu(dataset, model) -> float:\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    for src_text, ref_text in zip(wiki_test_pairs.source, wiki_test_pairs.target):\n",
    "        references.append(ref_text)\n",
    "        inputs = lang_module.tokenizer(src_text, truncation=True, return_tensors=\"pt\").to(test_device)\n",
    "        outputs = translator_model.generate(**inputs)\n",
    "        translations = lang_module.tokenizer.batch_decode(outputs, remove_special_tokens=True)\n",
    "        hypotheses.append(translations[0])\n",
    "\n",
    "    bleu = evaluator.evaluate_str(references, hypotheses)\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-distribution BLEUs: supervised & unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_id_sup = evaluate_bleu(opensub_test_pairs, supervised_model)\n",
    "print(\"Test BLEU of supervised model on Wiki: %s\" % bleu_id_sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_id_unsup = evaluate_bleu(wiki_test_pairs, unsupervised_model)\n",
    "print(\"Test BLEU of supervised model on Wiki: %s\" % bleu_id_unsup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-distribution BLEUs: supervised & unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_ood_sup = evaluate_bleu(opensub_test_pairs, supervised_model)\n",
    "print(\"Test BLEU of supervised model on Wiki: %s\" % bleu_ood_sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bleu_ood_unsup = evaluate_bleu(opensub_test_pairs, unsupervised_model)\n",
    "print(\"Test BLEU of supervised model on Wiki: %s\" % evaluate_bleu(opensub_test_pairs, unsupervised_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}